{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXr0rYYoxuFn"
      },
      "source": [
        "# **COSE474-2024F: Deep Learning HW2**\n",
        "### Student Name: 아이샤\n",
        "### Student ID: 2022320119"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uYvBsB1yHBs"
      },
      "source": [
        "## 0.1 Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKQwo4bCldFg",
        "outputId": "db74a14c-bcf7-493d-f292-fc1d72bc2002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch==2.0.0\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchvision==0.15.1\n",
            "  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.1) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.1) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.1) (10.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (3.30.5)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 455, in run\n",
            "    installed = install_given_reqs(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/__init__.py\", line 70, in install_given_reqs\n",
            "    requirement.install(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_install.py\", line 851, in install\n",
            "    install_wheel(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/install/wheel.py\", line 726, in install_wheel\n",
            "    _install_wheel(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/install/wheel.py\", line 584, in _install_wheel\n",
            "    file.save()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/install/wheel.py\", line 382, in save\n",
            "    shutil.copyfileobj(f, dest)\n",
            "  File \"/usr/lib/python3.10/shutil.py\", line 195, in copyfileobj\n",
            "    buf = fsrc_read(length)\n",
            "  File \"/usr/lib/python3.10/zipfile.py\", line 930, in read\n",
            "    data = self._read1(n)\n",
            "  File \"/usr/lib/python3.10/zipfile.py\", line 1006, in _read1\n",
            "    data = self._decompressor.decompress(data, n)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n",
            "    return self._cache[level]\n",
            "KeyError: 50\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n",
            "    if self.isEnabledFor(CRITICAL):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1736, in isEnabledFor\n",
            "    if self.manager.disable >= level:\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "pip install torch==2.0.0 torchvision==0.15.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrB5ikPZPaHG"
      },
      "outputs": [],
      "source": [
        "pip install d2l==1.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KafKuUVgTVji"
      },
      "source": [
        "## 7.1 From Fully Connected Layers to Convolutions\n",
        "\n",
        "This section introduces the limitations of using fully connected layers (MLPs) for high-dimensional perceptual data like images. It explains that convolutional neural networks (CNNs) can exploit the spatial structure of images, drastically reducing the number of parameters required compared to fully connected networks. CNNs take advantage of two key principles: translation invariance (ability to detect objects regardless of their position) and locality (focusing on local regions of the image for analysis). These properties make CNNs effective for tasks like image classification.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-061NLwTmZK"
      },
      "source": [
        "## 7.1.2 Constraining the MLP\n",
        "$$\n",
        "[\\mathbf{H}]_{i,j} = [\\mathbf{U}]_{i,j} + \\sum_{k} \\sum_{l} [\\mathbf{W}]_{i,j,k,l} [\\mathbf{X}]_{k,l}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= [\\mathbf{U}]_{i,j} + \\sum_{a} \\sum_{b} [\\mathbf{V}]_{i,j,a,b} [\\mathbf{X}]_{i+a,j+b}.\n",
        "$$\n",
        "\n",
        "This section discusses transforming fully connected layers into convolutional layers. By recognizing the spatial structure in images, we reduce the number of parameters from a fully connected layer to a convolutional one, resulting in more efficient models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpOJ25RwUfKM"
      },
      "source": [
        "### 7.1.2.1 Translation Invariance\n",
        "$$\n",
        "[\\mathbf{H}]_{i,j} = u + \\sum_{a} \\sum_{b} [\\mathbf{V}]_{a,b} [\\mathbf{X}]_{i+a,j+b}.\n",
        "$$\n",
        "\n",
        "Translation invariance ensures that a shift in the input image leads to a shift in the hidden representation, allowing the model to recognize objects regardless of location.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxIheqP8U9M_"
      },
      "source": [
        "### 7.1.2.2 Locality\n",
        "$$\n",
        "[\\mathbf{H}]_{i,j} = u + \\sum_{a=-\\Delta}^{\\Delta} \\sum_{b=-\\Delta}^{\\Delta} [\\mathbf{V}]_{a,b} [\\mathbf{X}]_{i+a,j+b}.\n",
        "$$\n",
        "\n",
        "Locality ensures that only local pixel information contributes to the output, reducing parameters by constraining filters to focus on nearby pixels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P47mXJ3aVHU1"
      },
      "source": [
        "## 7.1.3 Convolutions\n",
        "\n",
        "In this section, the mathematical concept of convolution is explained. Convolution measures the overlap between two functions \\( f \\) and \\( g \\), and is defined in continuous form using an integral:\n",
        "\n",
        "$$\n",
        "(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x} - \\mathbf{z}) d\\mathbf{z}\n",
        "$$\n",
        "\n",
        "For discrete objects, this becomes a sum:\n",
        "\n",
        "$$\n",
        "(f * g)(i) = \\sum_{a} f(a) g(i - a)\n",
        "$$\n",
        "\n",
        "For two-dimensional tensors, the sum extends to both dimensions:\n",
        "\n",
        "$$\n",
        "(f * g)(i, j) = \\sum_{a} \\sum_{b} f(a, b) g(i - a, j - b)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46DCUy3NV1Qc"
      },
      "source": [
        "### 7.1.4 Channels\n",
        "$$\n",
        "[\\mathbf{H}]_{i,j,d} = \\sum_{a=-\\Delta}^{\\Delta} \\sum_{b=-\\Delta}^{\\Delta} \\sum_{c} [\\mathbf{V}]_{a,b,c,d} [\\mathbf{X}]_{i+a,j+b,c}.\n",
        "$$\n",
        "\n",
        "Explains how images with multiple channels (e.g., RGB) are processed by convolutional layers, introducing 3D tensors and filters for each channel.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQD94l2mV9TU"
      },
      "source": [
        "## 7.2 Convolutions for images\n",
        "This section introduces how convolutional neural networks (CNNs) can efficiently process image data by leveraging convolutions. Instead of using fully connected layers, CNNs use convolutional layers that capture spatial structures, making them effective for image classification, object detection, and other vision-related tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPUpLwfCWBGb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wKBR5zBWOjQ"
      },
      "source": [
        "### 7.2.1 The Cross-Correlation Operation\n",
        "Cross-correlation is the operation typically used in convolutional layers, even though it is often referred to as \"convolution.\" The operation slides a kernel (filter) over the input data and performs elementwise multiplication between the input and the kernel, summing up the results to produce the output. The operation reduces the output size since it can only calculate values where the kernel fits within the image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa22lrt5WD4z"
      },
      "outputs": [],
      "source": [
        "def corr2d(X, K):\n",
        "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
        "    h, w = K.shape\n",
        "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiGU0_xAWHHO"
      },
      "outputs": [],
      "source": [
        "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
        "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
        "corr2d(X, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW6cwDcJWTxr"
      },
      "source": [
        "### 7.2.2 Convolutional Layers\n",
        "A convolutional layer performs the cross-correlation between the input and kernel, then adds a scalar bias. The kernel and bias are learnable parameters that are updated during training. The kernels are typically initialized randomly, and the model learns them through backpropagation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOarFCrrWK-8"
      },
      "outputs": [],
      "source": [
        "class Conv2D(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return corr2d(x, self.weight) + self.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8Xx3YQXWaTa"
      },
      "source": [
        "### 7.2.3 Object Edge Detection in Images\n",
        "Convolutional layers can detect edges in images by applying specific kernels, such as [1, -1], which can approximate a first derivative. When this kernel is applied to an image, it highlights the areas where pixel values change rapidly (edges), making it useful for edge detection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac-IzWY9WZN9"
      },
      "outputs": [],
      "source": [
        "X = torch.ones((6, 8))\n",
        "X[:, 2:6] = 0\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj1qXLRDWekv"
      },
      "outputs": [],
      "source": [
        "K = torch.tensor([[1.0, -1.0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrmVavkEWjKN"
      },
      "outputs": [],
      "source": [
        "Y = corr2d(X, K)\n",
        "Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqqjF_RNWl59"
      },
      "outputs": [],
      "source": [
        "corr2d(X.t(), K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZjS7vfgWo5D"
      },
      "source": [
        "### 7.2.4 Learning a Kernel\n",
        "Instead of manually designing kernels for tasks like edge detection, CNNs can learn the optimal kernels directly from data. The kernels are initialized randomly, and their values are adjusted during training by minimizing the loss function. This allows CNNs to automatically learn the filters that best capture features like edges, textures, and patterns from the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVLCHX6lWnXY"
      },
      "outputs": [],
      "source": [
        "# Construct a two-dimensional convolutional layer with 1 output channel and a\n",
        "# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\n",
        "conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)\n",
        "\n",
        "# The two-dimensional convolutional layer uses four-dimensional input and\n",
        "# output in the format of (example, channel, height, width), where the batch\n",
        "# size (number of examples in the batch) and the number of channels are both 1\n",
        "X = X.reshape((1, 1, 6, 8))\n",
        "Y = Y.reshape((1, 1, 6, 7))\n",
        "lr = 3e-2  # Learning rate\n",
        "\n",
        "for i in range(10):\n",
        "    Y_hat = conv2d(X)\n",
        "    l = (Y_hat - Y) ** 2\n",
        "    conv2d.zero_grad()\n",
        "    l.sum().backward()\n",
        "    # Update the kernel\n",
        "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
        "    if (i + 1) % 2 == 0:\n",
        "        print(f'epoch {i + 1}, loss {l.sum():.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ_L3eO4Wuvq"
      },
      "outputs": [],
      "source": [
        "conv2d.weight.data.reshape((1, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKWRkJjjXKKz"
      },
      "source": [
        "### 7.2.5 Cross-Correlation and Convolution\n",
        "This section explains the difference between cross-correlation (used in most deep learning frameworks) and strict convolution. Cross-correlation doesn't flip the kernel, while strict convolution does. Despite this difference, the learned kernels produce the same result because the kernel is learned directly from the data, whether or not it is flipped.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiLZYzEjXV3j"
      },
      "source": [
        "### 7.2.6 Feature Map and Receptive Field\n",
        "A feature map is the output of a convolutional layer, representing the learned features of the input data. The receptive field of an element in the feature map refers to the portion of the input that influences that element. As you stack convolutional layers in a deeper network, the receptive field of each element increases, allowing the network to capture more global information from the input image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DamA8o9bXjR1"
      },
      "source": [
        "## 7.3 Padding and Stride"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFw3L_-iXGXH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o-ja4nRXsOG"
      },
      "source": [
        "### 7.3.1 Padding\n",
        "Padding is a technique used in convolutional layers to prevent shrinking of the output size after multiple convolutions. It involves adding extra rows and columns (usually filled with zeros) around the boundary of an input image, ensuring that the kernel can be applied to every pixel, including the boundary pixels. This keeps the height and width of the output equal to that of the input, making it easier to predict output dimensions. Padding is especially useful when applying many convolution layers in succession.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWLFvYJYXUce"
      },
      "outputs": [],
      "source": [
        "# We define a helper function to calculate convolutions. It initializes the\n",
        "# convolutional layer weights and performs corresponding dimensionality\n",
        "# elevations and reductions on the input and output\n",
        "def comp_conv2d(conv2d, X):\n",
        "    # (1, 1) indicates that batch size and the number of channels are both 1\n",
        "    X = X.reshape((1, 1) + X.shape)\n",
        "    Y = conv2d(X)\n",
        "    # Strip the first two dimensions: examples and channels\n",
        "    return Y.reshape(Y.shape[2:])\n",
        "\n",
        "# 1 row and column is padded on either side, so a total of 2 rows or columns\n",
        "# are added\n",
        "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)\n",
        "X = torch.rand(size=(8, 8))\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddVh7nJdXxFO"
      },
      "outputs": [],
      "source": [
        "# We use a convolution kernel with height 5 and width 3. The padding on either\n",
        "# side of the height and width are 2 and 1, respectively\n",
        "conv2d = nn.LazyConv2d(1, kernel_size=(5, 3), padding=(2, 1))\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2ICsD5BX3T0"
      },
      "source": [
        "### 7.3.2 Stride\n",
        "Stride refers to the number of pixels by which the convolutional kernel is shifted across the input image. In the default case, the kernel slides over one pixel at a time, but by increasing the stride, we can reduce the size of the output. A stride greater than 1 skips over intermediate pixels, allowing for downsampling, which is useful for reducing the computational complexity of the model. A larger stride produces smaller outputs and can help in cases where the input size is too large to process efficiently.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXPJ8XbTYQ50"
      },
      "outputs": [],
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jnsp9EHfYUmh"
      },
      "outputs": [],
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMAG_P73YZ0i"
      },
      "source": [
        "## 7.4 Multiple Input and Multiple Output Channels\n",
        "\n",
        "This section delves into handling multiple input and output channels in Convolutional Neural Networks (CNNs). While earlier sections simplified examples by using single-channel inputs and outputs, real-world applications like RGB images inherently involve multiple channels. Here, both the input data and the convolutional kernels become three-dimensional tensors, incorporating the channel dimension alongside height and width. The section emphasizes the importance of aligning the number of input channels in the convolutional kernels with those in the input data to ensure proper cross-correlation operations. Additionally, it introduces the concept of multiple output channels, allowing CNNs to learn a diverse set of features by producing multiple feature maps from a single input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be-0JqZ7Ye56"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL-82D8WYgca"
      },
      "source": [
        "### 7.4.1 Multiple Input Channels\n",
        "When dealing with multi-channel input data (e.g., RGB images), convolutional kernels must have the same number of input channels. Each channel of the input interacts with its corresponding channel in the kernel, and the results are summed up to produce the final output. This helps capture features across multiple channels. For example, a convolution applied to an RGB image will combine information from all color channels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twcuREaLYfO0"
      },
      "outputs": [],
      "source": [
        "def corr2d_multi_in(X, K):\n",
        "    # Iterate through the 0th dimension (channel) of K first, then add them up\n",
        "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4JOqwpaYthL"
      },
      "outputs": [],
      "source": [
        "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
        "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
        "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
        "\n",
        "corr2d_multi_in(X, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDi8MnOKYxKK"
      },
      "source": [
        "### 7.4.2 Multiple Output Channels\n",
        "In practice, convolutional layers often have multiple output channels. Each output channel is the result of a separate convolution between the input and a unique set of filters. This increases the model's capacity to learn diverse features, with each output channel detecting different patterns in the input. As the network goes deeper, the number of output channels typically increases, allowing the model to learn more complex and hierarchical features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE8uxZrcYzlb"
      },
      "outputs": [],
      "source": [
        "def corr2d_multi_in_out(X, K):\n",
        "    # Iterate through the 0th dimension of K, and each time, perform\n",
        "    # cross-correlation operations with input X. All of the results are\n",
        "    # stacked together\n",
        "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TBL59OcY1ve"
      },
      "outputs": [],
      "source": [
        "K = torch.stack((K, K + 1, K + 2), 0)\n",
        "K.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYX2rAMfY3XG"
      },
      "outputs": [],
      "source": [
        "corr2d_multi_in_out(X, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBSDfaFXY7vR"
      },
      "source": [
        "### 7.4.3 $1\\times1$ Convolutional Layer\n",
        "A 1 × 1 convolution may seem counterintuitive at first, but it has practical uses. It operates on the channel dimension only, meaning that it transforms the input across different channels at each pixel location, but doesn't combine spatial information (height and width). This technique is used in more complex architectures to efficiently reduce or expand the number of channels while preserving the spatial resolution of the image. Essentially, it can be seen as a way to perform fully connected operations at every pixel, sharing weights across spatial dimensions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NVAPleWY5LZ"
      },
      "outputs": [],
      "source": [
        "def corr2d_multi_in_out_1x1(X, K):\n",
        "    c_i, h, w = X.shape\n",
        "    c_o = K.shape[0]\n",
        "    X = X.reshape((c_i, h * w))\n",
        "    K = K.reshape((c_o, c_i))\n",
        "    # Matrix multiplication in the fully connected layer\n",
        "    Y = torch.matmul(K, X)\n",
        "    return Y.reshape((c_o, h, w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbHhVL7JZ_R0"
      },
      "outputs": [],
      "source": [
        "X = torch.normal(0, 1, (3, 3, 3))\n",
        "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
        "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
        "Y2 = corr2d_multi_in_out(X, K)\n",
        "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykl8mt5-aEmE"
      },
      "source": [
        "## 7.5 Pooling\n",
        "Pooling is used in convolutional neural networks (CNNs) to reduce the spatial dimensions (height and width) of feature maps while retaining important information. Pooling layers are placed between convolutional layers, typically helping to make the model more robust to small translations in the input image. The main types of pooling are max-pooling and average pooling, and pooling is especially useful for downsampling and reducing the computational load.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxxT1CzTaAu1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkKifTyoszIs"
      },
      "source": [
        "### 7.5.1 Maximum Pooling and Average Pooling\n",
        "Pooling layers slide a fixed-size window over the input data, aggregating the values in the window. In max-pooling, the maximum value within each window is taken, while in average pooling, the average value is taken. Max-pooling is more commonly used since it provides better feature extraction by focusing on prominent features. Pooling reduces the spatial resolution of the input, making the network more invariant to small changes in the input, such as shifts in object positions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeBI-OC-aHmk"
      },
      "outputs": [],
      "source": [
        "def pool2d(X, pool_size, mode='max'):\n",
        "    p_h, p_w = pool_size\n",
        "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            if mode == 'max':\n",
        "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
        "            elif mode == 'avg':\n",
        "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qba9ZSXPaJeI"
      },
      "outputs": [],
      "source": [
        "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
        "pool2d(X, (2, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okuOWJhtaK0q"
      },
      "outputs": [],
      "source": [
        "pool2d(X, (2, 2), 'avg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qU3thUFaPHM"
      },
      "source": [
        "### 7.5.2 Padding and Stride\n",
        "Just like in convolution layers, padding and stride can be applied to pooling layers. Padding ensures that the pooling operation covers the borders of the input data. Stride controls how far the pooling window moves across the input, allowing for further downsampling. By default, the stride equals the size of the pooling window, but it can be manually adjusted to control the output size. This allows for more control over the pooling layer’s output resolution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JwaZ78oaMyB"
      },
      "outputs": [],
      "source": [
        "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4S0fKIgaSrC"
      },
      "outputs": [],
      "source": [
        "pool2d = nn.MaxPool2d(3)\n",
        "# Pooling has no model parameters, hence it needs no initialization\n",
        "pool2d(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn4y_LP5aUBV"
      },
      "outputs": [],
      "source": [
        "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
        "pool2d(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHxtNfyWaV6P"
      },
      "outputs": [],
      "source": [
        "pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n",
        "pool2d(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5P286U9ab1N"
      },
      "source": [
        "### 7.5.3 Multiple Channels\n",
        "In multi-channel inputs (like RGB images), the pooling layer applies the pooling operation to each channel separately. This preserves the number of channels from the input to the output. Pooling does not merge channels; it simply reduces the spatial dimensions independently for each channel. For example, applying pooling on a two-channel input will result in a two-channel output, with reduced spatial size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9l1Ix04aYWL"
      },
      "outputs": [],
      "source": [
        "X = torch.cat((X, X + 1), 1)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-ZE-mBAagf_"
      },
      "outputs": [],
      "source": [
        "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
        "pool2d(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1jhW8SNtatC"
      },
      "source": [
        "### Summary\n",
        "Pooling is a simple yet powerful technique for downsampling, improving translation invariance, and reducing computational load in CNNs. Max-pooling is generally preferred over average pooling for feature extraction. Strides and padding in pooling layers function similarly to how they do in convolutional layers. Lastly, pooling preserves the number of channels in multi-channel inputs, ensuring that the structural information is maintained across layers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk9o88wRalB1"
      },
      "source": [
        "## 7.6 Convolutional Neural Networks (LeNet)\n",
        "LeNet was one of the earliest and most influential convolutional neural networks (CNNs), introduced by Yann LeCun in the 1990s. It was designed primarily for handwritten digit recognition and played a significant role in the adoption of neural networks for computer vision tasks. LeNet consists of two key parts: a convolutional encoder (with two convolutional layers) and a dense block (three fully connected layers). This structure allows the network to effectively capture spatial information in images while reducing the number of parameters compared to fully connected layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fezF1NCwah3D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QEwufEasN0"
      },
      "source": [
        "### 7.6.1 LeNet\n",
        "LeNet is composed of:\n",
        "\n",
        "1. Convolutional Layers: Two convolutional layers, each followed by a sigmoid activation function and an average pooling operation. These convolutional layers capture local spatial information while downsampling the input.\n",
        "2. Pooling Operations: Pooling layers reduce the resolution of the image, making the network more invariant to small shifts in the input.\n",
        "3. Fully Connected Layers: After the convolutional layers, the feature maps are flattened into a vector to be processed by fully connected layers, producing a final classification. The last layer outputs probabilities for 10 possible outcomes (in the case of digit classification).\n",
        "\n",
        "The network's key innovation was reducing the size of the model by using convolution and pooling, making it feasible to apply neural networks to image data in a more efficient manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noIcbyNXarUM"
      },
      "outputs": [],
      "source": [
        "def init_cnn(module):\n",
        "    \"\"\"Initialize weights for CNNs.\"\"\"\n",
        "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
        "        nn.init.xavier_uniform_(module.weight)\n",
        "\n",
        "class LeNet(d2l.Classifier):\n",
        "    \"\"\"The LeNet-5 model.\"\"\"\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120), nn.Sigmoid(),\n",
        "            nn.LazyLinear(84), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKc2_Lnpavkq"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(d2l.Classifier)\n",
        "def layer_summary(self, X_shape):\n",
        "    X = torch.randn(*X_shape)\n",
        "    for layer in self.net:\n",
        "        X = layer(X)\n",
        "        print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n",
        "\n",
        "model = LeNet()\n",
        "model.layer_summary((1, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb1ZKMvJa-Vt"
      },
      "source": [
        "### 7.6.2 Training\n",
        "LeNet is typically trained using the Fashion-MNIST dataset, which consists of small grayscale images. The model is trained with cross-entropy loss and stochastic gradient descent. Despite having fewer parameters than a fully connected network, CNNs like LeNet require more computation per parameter due to the convolution operations.\n",
        "\n",
        "Training involves initializing the model's parameters and then running the training process for several epochs. While LeNet is relatively simple by modern standards, it was groundbreaking at the time and remains a good starting point for understanding CNNs. The model’s success also showed how CNNs could outperform traditional machine learning models in visual tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-3BidDka6Ip"
      },
      "outputs": [],
      "source": [
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128)\n",
        "model = LeNet(lr=0.1)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUYIh82pccWd"
      },
      "source": [
        "## 8.2 Networks Using Blocks (VGG)\n",
        "The VGG network introduced the idea of using blocks of layers as reusable structures within a deep neural network. Instead of designing each layer individually, VGG uses repeated blocks of layers with convolutional and pooling operations, making the network deeper and more structured. This concept of modular blocks became a foundational principle in deep network design, enabling researchers to build deeper networks more systematically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2F1hRXjbBz7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H85RMYZBchNl"
      },
      "source": [
        "### 8.2.1 VGG Blocks\n",
        "A VGG block consists of a sequence of convolutional layers with small\n",
        "3×3 kernels (with padding to maintain the resolution), followed by a nonlinearity (ReLU), and finally a max-pooling layer that halves the height and width. The use of smaller kernels stacked together helps extract more complex features without drastically increasing the number of parameters. Each block in the VGG network is designed to downsample the spatial dimensions while increasing the number of feature maps (channels).\n",
        "\n",
        "The function **vgg_block()** in the code defines such a block, allowing for easy customization of the number of convolutional layers and output channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZl3LrxKck5Z"
      },
      "outputs": [],
      "source": [
        "def vgg_block(num_convs, out_channels):\n",
        "    layers = []\n",
        "    for _ in range(num_convs):\n",
        "        layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\n",
        "        layers.append(nn.ReLU())\n",
        "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
        "    return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FDFMm16cmhO"
      },
      "source": [
        "### 8.2.2 VGG Network\n",
        "The VGG network was a major breakthrough because it showed that deep networks, with many layers of convolutions, performed better than shallower, wider ones. VGG-11, for instance, has 11 layers (8 convolutional layers and 3 fully connected layers). The VGG family of networks includes other configurations, such as VGG-16 and VGG-19, which use more convolutional layers for better accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btpy35QrcgYR"
      },
      "outputs": [],
      "source": [
        "class VGG(d2l.Classifier):\n",
        "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        conv_blks = []\n",
        "        for (num_convs, out_channels) in arch:\n",
        "            conv_blks.append(vgg_block(num_convs, out_channels))\n",
        "        self.net = nn.Sequential(\n",
        "            *conv_blks, nn.Flatten(),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.LazyLinear(num_classes))\n",
        "        self.net.apply(d2l.init_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZS63Gv2dcTV"
      },
      "outputs": [],
      "source": [
        "VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(\n",
        "    (1, 1, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEYNARsedfqD"
      },
      "source": [
        "### 8.2.3 Training\n",
        "VGG is computationally more expensive than AlexNet due to the deeper architecture and larger number of parameters. Therefore, for practical purposes, a smaller VGG variant (with fewer output channels) can be used to train on datasets like Fashion-MNIST. The training process follows similar steps to AlexNet, using mini-batch stochastic gradient descent, but VGG requires more computational power due to the additional layers.\n",
        "\n",
        "The key insight from VGG is that deeper networks can extract more complex patterns and lead to better performance in tasks like image recognition. However, the increase in depth comes at the cost of higher computation and memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y07ZavideI6"
      },
      "outputs": [],
      "source": [
        "model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E82jitDsXtA"
      },
      "source": [
        "## 8.6 Residual Networks (ResNet) and ResNeXt\n",
        "As neural networks grow deeper, adding more layers doesn't always improve performance due to vanishing/exploding gradients and other training issues. Residual Networks (ResNet) were designed to address this by making it easier for networks to learn identity mappings. ResNet's key innovation is the \"residual block,\" which allows layers to skip connections and pass their input directly to deeper layers. This allows for much deeper networks to be trained effectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JT-Yl_BsTRR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6EpJPKesgQo"
      },
      "source": [
        "### 8.6.1 Function Classes\n",
        "$$ f^*_{\\mathcal{F}} = \\underset{f}{\\mathrm{argmin}} \\, L(\\mathbf{X}, \\mathbf{y}, f) \\, \\text{subject to} \\, f \\in \\mathcal{F} $$\n",
        "\n",
        "### LaTeX for the equation:\n",
        "```latex\n",
        "f^*_{\\mathcal{F}} = \\underset{f}{\\mathrm{argmin}} \\, L(\\mathbf{X}, \\mathbf{y}, f) \\, \\text{subject to} \\, f \\in \\mathcal{F}.\n",
        "```\n",
        "\n",
        "This equation represents the optimization problem where we are trying to find the best function $ f^*_{\\mathcal{F}} $ within a class of functions $ \\mathcal{F} $ that minimizes the loss function $ L(\\mathbf{X}, \\mathbf{y}, f) $ for given data features $ \\mathbf{X} $ and labels $\\mathbf{y}$. The class $ \\mathcal{F}$ consists of all functions that a particular neural network architecture can approximate, given its parameters and hyperparameters. The goal is to find the best approximation of the true function $ f^* $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC7o7bR1tbM5"
      },
      "source": [
        "### 8.6.2 Residual Blocks\n",
        "Residual blocks are the core building blocks of ResNet. Instead of directly learning a function\n",
        "F(x), residual blocks aim to learn the \"residual\"\n",
        "F(x)−x. The key idea is that layers in a residual block learn the difference between the input and the output, allowing for more stable training.\n",
        "\n",
        "In a regular block, the network must learn the direct transformation. In a residual block, the network learns the residual, which simplifies the learning process when the optimal transformation is close to the identity function. If the network doesn't need the extra transformation, the added layers in a residual block can learn the identity function and pass the input directly through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vngD1Z5gsTto"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
        "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
        "                                   stride=strides)\n",
        "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
        "                                       stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8YTfv7Ethdq"
      },
      "outputs": [],
      "source": [
        "blk = Residual(3)\n",
        "X = torch.randn(4, 3, 6, 6)\n",
        "blk(X).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-wi-aPitjz9"
      },
      "outputs": [],
      "source": [
        "blk = Residual(6, use_1x1conv=True, strides=2)\n",
        "blk(X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKUjlDHWtmoz"
      },
      "source": [
        "### 8.6.3 ResNet Model\n",
        "ResNet models are built by stacking residual blocks. The first layers are the same as traditional convolutional networks (e.g., a\n",
        "7×7 convolution followed by a max-pooling layer). Then, residual blocks are added in groups, with each group potentially reducing the spatial resolution while increasing the number of channels.\n",
        "\n",
        "\n",
        "In ResNet-18, for example, there are four groups of residual blocks, with each block doubling the number of channels while halving the spatial resolution. The final output is passed through a global average pooling layer and a fully connected layer for classification.\n",
        "\n",
        "\n",
        "The architecture allows ResNet to handle very deep networks (e.g., ResNet-152) without suffering from the degradation problem that typically affects deeper networks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su-zynNHtlKy"
      },
      "outputs": [],
      "source": [
        "class ResNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUGMjDqwttku"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(ResNet)\n",
        "def block(self, num_residuals, num_channels, first_block=False):\n",
        "    blk = []\n",
        "    for i in range(num_residuals):\n",
        "        if i == 0 and not first_block:\n",
        "            blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(Residual(num_channels))\n",
        "    return nn.Sequential(*blk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3c6zpPutv8m"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(ResNet)\n",
        "def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.net = nn.Sequential(self.b1())\n",
        "    for i, b in enumerate(arch):\n",
        "        self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
        "    self.net.add_module('last', nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "        nn.LazyLinear(num_classes)))\n",
        "    self.net.apply(d2l.init_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoIasp6ztxj7"
      },
      "outputs": [],
      "source": [
        "class ResNet18(ResNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\n",
        "                       lr, num_classes)\n",
        "\n",
        "ResNet18().layer_summary((1, 1, 96, 96))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7OcmYTju1_n"
      },
      "source": [
        "### 8.6.4 Training\n",
        "ResNet’s effectiveness lies in its ability to train very deep networks efficiently. The use of residual connections allows for easier gradient flow and better convergence during training, as the identity mapping can be learned if needed.\n",
        "\n",
        "ResNet-18, for instance, can be trained on datasets like Fashion-MNIST, using techniques like mini-batch stochastic gradient descent. The architecture, with its residual connections, ensures that the model does not overfit easily, although deeper versions of ResNet might require larger datasets for optimal performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6goWLcVst046"
      },
      "outputs": [],
      "source": [
        "model = ResNet18(lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgfZVFLcwtyp"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWf4sDhpww1t"
      },
      "source": [
        "### 7.1.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMeNjEACwvTo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Create a 1D convolution for audio signal\n",
        "audio_signal = torch.randn(1, 1, 100)  # Batch size 1, 1 channel, length 100\n",
        "conv1d = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "# Apply convolution\n",
        "output_signal = conv1d(audio_signal)\n",
        "print(\"Audio signal after convolution: \", output_signal.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ93xQMJxLcj"
      },
      "source": [
        "### 7.2.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deyQN0uuw8cT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a directional edge-detection kernel\n",
        "kernel = torch.tensor([[-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0]])  # Sobel-like kernel\n",
        "kernel = kernel.unsqueeze(0).unsqueeze(0)  # Shape (1, 1, 3, 3)\n",
        "\n",
        "# Image with diagonal edges\n",
        "image = torch.randn(1, 1, 5, 5)\n",
        "\n",
        "# Apply convolution\n",
        "edge_detected = F.conv2d(image, kernel, padding=1)\n",
        "print(\"Edge-detected output shape: \", edge_detected.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym0Xfy-oxR_K"
      },
      "source": [
        "### 7.3.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peYRaH4Tw22F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define a convolutional layer with specific kernel size, padding, and stride\n",
        "conv2d = torch.nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
        "\n",
        "# Input tensor (random image)\n",
        "X = torch.rand(size=(1, 1, 10, 10))\n",
        "\n",
        "# Apply convolution\n",
        "output = conv2d(X)\n",
        "print(\"Output shape with kernel (3, 5), padding (0, 1), and stride (3, 4):\", output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnbpWvDlxUrH"
      },
      "source": [
        "### 7.4.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9mlo0jQw-5V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define two convolution kernels\n",
        "conv1 = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
        "conv2 = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
        "\n",
        "# Input tensor\n",
        "X = torch.rand(size=(1, 1, 10, 10))\n",
        "\n",
        "# Apply convolutions\n",
        "output1 = conv1(X)\n",
        "output2 = conv2(output1)\n",
        "\n",
        "print(\"Output shape after applying two convolutions: \", output2.shape)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}