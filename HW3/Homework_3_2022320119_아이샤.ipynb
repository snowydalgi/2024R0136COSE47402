{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5Jm3UN_Hfsu"
      },
      "source": [
        "## **Homework 3**\n",
        "**Instructions**\n",
        "* This homework focuses on understanding and applying DETR for object detection and attention visualization. It consists of **three questions** designed to assess both theoretical understanding and practical application.\n",
        "\n",
        "* Please organize your answers and results for the questions below and submit this jupyter notebook as **a .pdf file**.\n",
        "\n",
        "* **Deadline: 11/14 (Thur) 23:59**\n",
        "\n",
        "**Reference**\n",
        "* End-to-End Object Detection with Transformers (DETR): https://github.com/facebookresearch/detr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3n9blo4JO7m"
      },
      "source": [
        "### **Q1.  Understanding DETR model**\n",
        "\n",
        "* Fill-in-the-blank exercise to test your understanding of critical parts of the DETR model workflow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SONlVIhPH_qF"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "class DETR(nn.Module):\n",
        "    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n",
        "                 num_encoder_layers=6, num_decoder_layers=6, num_queries=100):\n",
        "        super().__init__()\n",
        "\n",
        "        # create ResNet-50 backbone\n",
        "        self.backbone = resnet50()\n",
        "        del self.backbone.fc\n",
        "\n",
        "        # create conversion layer\n",
        "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
        "\n",
        "        # create a default PyTorch transformer\n",
        "        self.transformer = nn.Transformer(\n",
        "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
        "\n",
        "        # prediction heads, one extra class for predicting non-empty slots\n",
        "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
        "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # output positional encodings (object queries)\n",
        "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
        "\n",
        "        # spatial positional encodings\n",
        "        # note that in baseline DETR we use sine positional encodings\n",
        "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
        "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # propagate inputs through ResNet-50 up to avg-pool layer\n",
        "        x = self.backbone.conv1(inputs)\n",
        "        x = self.backbone.bn1(x)\n",
        "        x = self.backbone.relu(x)\n",
        "        x = self.backbone.maxpool(x)\n",
        "\n",
        "        x = self.backbone.layer1(x)\n",
        "        x = self.backbone.layer2(x)\n",
        "        x = self.backbone.layer3(x)\n",
        "        x = self.backbone.layer4(x)\n",
        "\n",
        "        # convert from 2048 to 256 feature planes for the transformer\n",
        "        h = self.conv(x)\n",
        "\n",
        "        # construct positional encodings\n",
        "        H, W = h.shape[-2:]\n",
        "        pos = torch.cat([\n",
        "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
        "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
        "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
        "\n",
        "        # propagate through the transformer\n",
        "        h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),\n",
        "                             self.query_pos.unsqueeze(1)).transpose(0, 1)\n",
        "\n",
        "\n",
        "\n",
        "        # finally project transformer outputs to class labels and bounding boxes\n",
        "        pred_logits = self.linear_class(h),\n",
        "        pred_boxes = self.linear_bbox(h).sigmoid()\n",
        "\n",
        "        return {'pred_logits': pred_logits,\n",
        "                'pred_boxes': pred_boxes}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CGZlqo-HtRN"
      },
      "source": [
        "### **Q2. Custom Image Detection and Attention Visualization**\n",
        "\n",
        "In this task, you will upload an **image of your choice** (different from the provided sample) and follow the steps below:\n",
        "\n",
        "* Object Detection using DETR\n",
        " * Use the DETR model to detect objects in your uploaded image.\n",
        "\n",
        "* Attention Visualization in Encoder\n",
        " * Visualize the regions of the image where the encoder focuses the most.\n",
        "\n",
        "* Decoder Query Attention in Decoder\n",
        " * Visualize how the decoder’s query attends to specific areas corresponding to the detected objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExsO0XgWHsvP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "from torchvision.models import resnet50\n",
        "import torchvision.transforms as T\n",
        "torch.set_grad_enabled(False);\n",
        "\n",
        "# COCO classes\n",
        "CLASSES = [\n",
        "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
        "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
        "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
        "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
        "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
        "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush'\n",
        "]\n",
        "\n",
        "# colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "# standard PyTorch mean-std input image normalization\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# for output bounding box post-processing\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b\n",
        "\n",
        "def plot_results(pil_img, prob, boxes):\n",
        "    plt.figure(figsize=(16,10))\n",
        "    plt.imshow(pil_img)\n",
        "    ax = plt.gca()\n",
        "    colors = COLORS * 100\n",
        "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                   fill=False, color=c, linewidth=3))\n",
        "        cl = p.argmax()\n",
        "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
        "        ax.text(xmin, ymin, text, fontsize=15,\n",
        "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TSPybYnIwSE"
      },
      "source": [
        "\n",
        "In this section, we show-case how to load a model from hub, run it on a custom image, and print the result.\n",
        "Here we load the simplest model (DETR-R50) for fast inference. You can swap it with any other model from the model zoo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aX2QNanH9T0"
      },
      "outputs": [],
      "source": [
        "model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
        "model.eval();\n",
        "\n",
        "url = 'https://www.chantillyanimalhospital.com/wp-content/uploads/2018/12/white_cat_and_dog.jpg'\n",
        "im = Image.open(requests.get(url, stream=True).raw) # put your own image\n",
        "\n",
        "# mean-std normalize the input image (batch-size: 1)\n",
        "img = transform(im).unsqueeze(0)\n",
        "\n",
        "# propagate through the model\n",
        "outputs = model(img)\n",
        "\n",
        "# keep only predictions with 0.7+ confidence\n",
        "probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "keep = probas.max(-1).values > 0.9\n",
        "\n",
        "# convert boxes from [0; 1] to image scales\n",
        "bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
        "\n",
        "# mean-std normalize the input image (batch-size: 1)\n",
        "img = transform(im).unsqueeze(0)\n",
        "\n",
        "# propagate through the model\n",
        "outputs = model(img)\n",
        "\n",
        "# keep only predictions with 0.7+ confidence\n",
        "probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "keep = probas.max(-1).values > 0.9\n",
        "\n",
        "# convert boxes from [0; 1] to image scales\n",
        "bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
        "\n",
        "# mean-std normalize the input image (batch-size: 1)\n",
        "img = transform(im).unsqueeze(0)\n",
        "\n",
        "# propagate through the model\n",
        "outputs = model(img)\n",
        "\n",
        "# keep only predictions with 0.7+ confidence\n",
        "probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "keep = probas.max(-1).values > 0.9\n",
        "\n",
        "# convert boxes from [0; 1] to image scales\n",
        "bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
        "\n",
        "plot_results(im, probas[keep], bboxes_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4alpH62I5GS"
      },
      "source": [
        "\n",
        "Here we visualize attention weights of the last decoder layer. This corresponds to visualizing, for each detected objects, which part of the image the model was looking at to predict this specific bounding box and class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCNmOSjGH9WV"
      },
      "outputs": [],
      "source": [
        "# use lists to store the outputs via up-values\n",
        "conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n",
        "\n",
        "hooks = [\n",
        "    model.backbone[-2].register_forward_hook(\n",
        "        lambda self, input, output: conv_features.append(output)\n",
        "    ),\n",
        "    model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n",
        "        lambda self, input, output: enc_attn_weights.append(output[1])\n",
        "    ),\n",
        "    model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n",
        "        lambda self, input, output: dec_attn_weights.append(output[1])\n",
        "    ),\n",
        "]\n",
        "\n",
        "# propagate through the model\n",
        "outputs = model(img) # put your own image\n",
        "\n",
        "for hook in hooks:\n",
        "    hook.remove()\n",
        "\n",
        "# don't need the list anymore\n",
        "conv_features = conv_features[0]\n",
        "enc_attn_weights = enc_attn_weights[0]\n",
        "dec_attn_weights = dec_attn_weights[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrrWPnotI_G-"
      },
      "outputs": [],
      "source": [
        "# get the feature map shape\n",
        "h, w = conv_features['0'].tensors.shape[-2:]\n",
        "\n",
        "fig, axs = plt.subplots(ncols=len(bboxes_scaled), nrows=2, figsize=(22, 7))\n",
        "colors = COLORS * 100\n",
        "for idx, ax_i, (xmin, ymin, xmax, ymax) in zip(keep.nonzero(), axs.T, bboxes_scaled):\n",
        "    ax = ax_i[0]\n",
        "    ax.imshow(dec_attn_weights[0, idx].view(h, w))\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'query id: {idx.item()}')\n",
        "    ax = ax_i[1]\n",
        "    ax.imshow(im)\n",
        "    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                               fill=False, color='blue', linewidth=3))\n",
        "    ax.axis('off')\n",
        "    ax.set_title(CLASSES[probas[idx].argmax()])\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBaCqpWcJBRz"
      },
      "outputs": [],
      "source": [
        "# output of the CNN\n",
        "f_map = conv_features['0']\n",
        "print(\"Encoder attention:      \", enc_attn_weights[0].shape)\n",
        "print(\"Feature map:            \", f_map.tensors.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th7dtFsDHsyh"
      },
      "outputs": [],
      "source": [
        "# get the HxW shape of the feature maps of the CNN\n",
        "shape = f_map.tensors.shape[-2:]\n",
        "# and reshape the self-attention to a more interpretable shape\n",
        "sattn = enc_attn_weights[0].reshape(shape + shape)\n",
        "print(\"Reshaped self-attention:\", sattn.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eRkeRezH8fX"
      },
      "outputs": [],
      "source": [
        "# downsampling factor for the CNN, is 32 for DETR and 16 for DETR DC5\n",
        "fact = 32\n",
        "\n",
        "# let's select 4 reference points for visualization\n",
        "idxs = [(200, 200), (280, 400), (200, 600), (300, 800),]\n",
        "\n",
        "# here we create the canvas\n",
        "fig = plt.figure(constrained_layout=True, figsize=(25 * 0.7, 8.5 * 0.7))\n",
        "# and we add one plot per reference point\n",
        "gs = fig.add_gridspec(2, 4)\n",
        "axs = [\n",
        "    fig.add_subplot(gs[0, 0]),\n",
        "    fig.add_subplot(gs[1, 0]),\n",
        "    fig.add_subplot(gs[0, -1]),\n",
        "    fig.add_subplot(gs[1, -1]),\n",
        "]\n",
        "\n",
        "# for each one of the reference points, let's plot the self-attention\n",
        "# for that point\n",
        "for idx_o, ax in zip(idxs, axs):\n",
        "    idx = (idx_o[0] // fact, idx_o[1] // fact)\n",
        "    ax.imshow(sattn[..., idx[0], idx[1]], cmap='cividis', interpolation='nearest')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'self-attention{idx_o}')\n",
        "\n",
        "\n",
        "# and now let's add the central image, with the reference points as red circles\n",
        "fcenter_ax = fig.add_subplot(gs[:, 1:-1])\n",
        "fcenter_ax.imshow(im)\n",
        "for (y, x) in idxs:\n",
        "    scale = im.height / img.shape[-2]\n",
        "    x = ((x // fact) + 0.5) * fact\n",
        "    y = ((y // fact) + 0.5) * fact\n",
        "    fcenter_ax.add_patch(plt.Circle((x * scale, y * scale), fact // 2, color='r'))\n",
        "    fcenter_ax.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bcx4iX5H1od"
      },
      "source": [
        "### **Q3. Understanding Attention Mechanisms**\n",
        "\n",
        "In this task, you focus on understanding the attention mechanisms present in the encoder and decoder of DETR.\n",
        "\n",
        "* Briefly describe the types of attention used in the encoder and decoder, and explain the key differences between them.\n",
        "\n",
        "**Encoder:** Self-Attention\n",
        "The encoder in DETR uses self-attention. This means each part (or patch) of the image pays attention to every other part. Self-attention in the encoder enables the model to capture global context, helping it understand the full scene and the relationships between different areas of the image.\n",
        "\n",
        "**Decoder:** Self-Attention and Cross-Attention\n",
        "The decoder has two types of attention:\n",
        "\n",
        "Self-Attention (between object queries): In the decoder, self-attention allows each object query to interact with other queries, which ensures that each detected object is treated separately. This helps avoid duplicate or overlapping detections.\n",
        "Cross-Attention (between object queries and encoder output): Cross-attention enables each object query to focus on specific parts of the image as represented by the encoder. This allows the model to identify the exact location of each object based on what the encoder \"sees.\"\n",
        "Key Differences\n",
        "\n",
        "Encoder Self-Attention focuses on the whole image, capturing a broad, global view of the scene.\n",
        "Decoder Self-Attention keeps each object unique, ensuring that the model handles multiple objects separately.\n",
        "Decoder Cross-Attention focuses on specific regions of the image to locate objects accurately.\n",
        "\n",
        ".\n",
        "* Based on the visualized results from Q2, provide an analysis of the distinct characteristics of each attention mechanism in the encoder and decoder. Feel free to express your insights.\n",
        "\n",
        "**Encoder Attention** Characteristics\n",
        "\n",
        "In the encoder’s attention maps, attention is often spread across the entire image. This broad spread indicates that the encoder is capturing the full scene, not just focusing on specific objects. It gathers information about the background and the relationships between different areas, giving the model an understanding of the entire scene.\n",
        "\n",
        "**Decoder Attention** Characteristics\n",
        "\n",
        "Self-Attention: The decoder’s self-attention ensures that object queries interact with each other, making each detected object distinct. This helps the model avoid confusion when objects are close together, ensuring each object stands out individually.\n",
        "Cross-Attention: Cross-attention in the decoder focuses on specific regions in the image where objects are likely to be. This focused attention helps the model pinpoint objects accurately, showing that the decoder can direct its attention to the relevant parts of the image to identify and locate objects.\n",
        "\n",
        "**Insights**\n",
        "\n",
        "The encoder’s self-attention provides a wide, scene-wide understanding of the image, which helps the model comprehend the context. In contrast, the decoder uses self- and cross-attention to identify and distinguish objects with precision. Together, these attention mechanisms allow DETR to balance a broad view of the scene with focused, accurate object detection."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}