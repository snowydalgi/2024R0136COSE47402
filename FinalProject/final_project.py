# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KYNjs0IH941KkEugU-vhsPU7EJQT_gS6
"""

!pip install transformers datasets matplotlib
!pip install torchvision scikit-learn

# Import libraries
from transformers import CLIPProcessor, CLIPModel
from torchvision import transforms, models, datasets
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
from torch import nn, optim
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from PIL import Image, UnidentifiedImageError
import torch
import matplotlib.pyplot as plt
from collections import Counter
from glob import glob
import os
import numpy as np
import hashlib
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

# Define dataset paths
train_path = "/content/drive/MyDrive/finalproject/dataset/train" #based on my file path to train
test_path = "/content/drive/MyDrive/finalproject/dataset/test" #based on my file path to test
categories = ["shirt", "dress", "shoes"]

# Visualize dataset with descriptions
for category in categories:
    images = glob(f"{train_path}/{category}/*.*")
    print(f"Category: {category}, Samples: {len(images)}")
    for img_path in images[:3]:  # Display 3 images per category
        img = Image.open(img_path)
        plt.imshow(img)
        plt.axis('off')
        plt.title(f"{category.capitalize()} - Example Image")
        plt.show()

def hash_image(image_path):
    with open(image_path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()

train_images = [hash_image(img) for img in glob(f"{train_path}/*/*")]
test_images = [hash_image(img) for img in glob(f"{test_path}/*/*")]

duplicates = set(train_images) & set(test_images)
if duplicates:
    print(f"Found {len(duplicates)} duplicate images between train and test!")
else:
    print("No duplicates found.")

from torchvision.transforms import functional as TF

# Define the augmentation function
augmentation_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Apply augmentations and save augmented images
def augment_image(image_path):
    try:
        image = Image.open(image_path).convert("RGB")
        augmented_image = augmentation_transform(image)
        augmented_image = TF.to_pil_image(augmented_image)
        save_path = image_path.replace("train", "augmented_train")
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        augmented_image.save(save_path)
    except Exception as e:
        print(f"Error augmenting {image_path}: {e}")

from multiprocessing import Pool

with Pool(4) as p:  # Adjust number of processes
    p.map(augment_image, glob(f"{train_path}/**/*.*", recursive=True))

# Define transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load datasets
train_dataset = ImageFolder(root=train_path, transform=transform)
test_dataset = ImageFolder(root=test_path, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Class distribution
counts = Counter(train_dataset.targets)
print("Class distribution:", counts)

clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").cuda()
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
prompts = [f"A photo of a {category}" for category in categories]

def predict_clip(image_path, prompts):
    image = Image.open(image_path)
    inputs = processor(text=prompts, images=image, return_tensors="pt", padding=True).to("cuda")
    outputs = clip_model(**inputs)
    logits_per_image = outputs.logits_per_image
    predicted_idx = logits_per_image.argmax().item()
    return prompts[predicted_idx].split()[-1]

# Test CLIP predictions
for img_path in glob(f"{test_path}/*/*.jpg")[:5]:
    print(f"Image: {img_path}, Predicted: {predict_clip(img_path, prompts)}")

resnet_model = models.resnet18(pretrained=True)
resnet_model.fc = nn.Linear(resnet_model.fc.in_features, len(train_dataset.classes))
resnet_model = resnet_model.cuda()

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(resnet_model.parameters(), lr=0.001)

# Train ResNet
num_epochs = 10
for epoch in range(num_epochs):
    resnet_model.train()
    running_loss, correct, total = 0.0, 0, 0

    for images, labels in train_loader:
        images, labels = images.cuda(), labels.cuda()
        optimizer.zero_grad()
        outputs = resnet_model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

    train_accuracy = 100 * correct / total
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Accuracy: {train_accuracy:.2f}%")

# Evaluate ResNet
resnet_model.eval()
y_true, y_pred = [], []

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.cuda(), labels.cuda()
        outputs = resnet_model(images)
        _, predicted = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

# Metrics
accuracy_resnet = accuracy_score(y_true, y_pred)
f1_score_resnet = f1_score(y_true, y_pred, average="weighted")
print(f"ResNet18 Accuracy: {accuracy_resnet * 100:.2f}%, F1-Score: {f1_score_resnet:.2f}")

# Evaluate CLIP
y_true_clip, y_pred_clip = [], []
for img_path in glob(f"{test_path}/*/*.jpg"):
    true_label = os.path.basename(os.path.dirname(img_path))
    predicted_label = predict_clip(img_path, prompts)
    y_true_clip.append(categories.index(true_label))
    y_pred_clip.append(categories.index(predicted_label))

accuracy_clip = accuracy_score(y_true_clip, y_pred_clip)
f1_score_clip = f1_score(y_true_clip, y_pred_clip, average="weighted")
print(f"CLIP Accuracy: {accuracy_clip * 100:.2f}%, F1-Score: {f1_score_clip:.2f}")

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=train_dataset.classes)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Visualization
data_iter = iter(test_loader)
images, labels = next(data_iter)
images = images.cuda()
outputs = resnet_model(images)
_, predicted = torch.max(outputs, 1)

plt.figure(figsize=(12, 8))
for idx in range(len(images)):
    plt.subplot(2, 4, idx + 1)
    plt.imshow(images[idx].permute(1, 2, 0).cpu().numpy())
    plt.title(f"True: {test_dataset.classes[labels[idx]]}\nPred: {test_dataset.classes[predicted[idx]]}")
    plt.axis("off")
plt.tight_layout()
plt.show()

